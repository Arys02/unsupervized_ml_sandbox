{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-18T10:42:45.880995Z",
     "start_time": "2025-07-18T10:42:44.296131Z"
    }
   },
   "source": "import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ndef load_dataset(dataset_path, max_samples_per_class=50):\n    \"\"\"Load dataset with error handling\"\"\"\n    images = []\n    labels = []\n    class_names = []\n\n    if not os.path.exists(dataset_path):\n        return generate_synthetic_data(), [], []\n\n    class_dirs = sorted([d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))])\n\n    for class_idx, class_name in enumerate(class_dirs):\n        class_path = os.path.join(dataset_path, class_name)\n        class_names.append(class_name)\n        \n        image_files = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n        loaded_count = 0\n\n        for image_file in image_files:\n            if loaded_count >= max_samples_per_class:\n                break\n                \n            image_path = os.path.join(class_path, image_file)\n            \n            try:\n                if os.path.getsize(image_path) == 0:\n                    continue\n                    \n                img = cv2.imread(image_path)\n                if img is not None:\n                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                    img_resized = cv2.resize(img_rgb, (64, 64))\n                    images.append(img_resized)\n                    labels.append(class_idx)\n                    loaded_count += 1\n            except:\n                continue\n\n    if len(images) == 0:\n        return generate_synthetic_data()\n    \n    return np.array(images), np.array(labels), class_names\n\ndef generate_synthetic_data():\n    \"\"\"Generate synthetic card data\"\"\"\n    images = []\n    labels = []\n    \n    for card_idx in range(13):\n        for suit_idx in range(4):\n            for variation in range(20):\n                img = np.zeros((64, 64, 3), dtype=np.uint8)\n                \n                base_color = [200, 50, 50] if suit_idx < 2 else [50, 50, 50]\n                \n                if card_idx == 0:\n                    img[25:40, 25:40] = base_color\n                elif card_idx > 9:\n                    img[15:50, 20:45] = base_color\n                    img[20:30, 25:40] = [255, 255, 255]\n                else:\n                    for i in range(min(card_idx + 1, 8)):\n                        y = 10 + (i % 4) * 12\n                        x = 15 + (i // 4) * 25\n                        img[y:y+8, x:x+8] = base_color\n                \n                noise = np.random.normal(0, 15, (64, 64, 3))\n                img = np.clip(img.astype(float) + noise, 0, 255).astype(np.uint8)\n                \n                images.append(img)\n                labels.append(card_idx * 4 + suit_idx)\n    \n    return np.array(images), np.array(labels)\n\n# Load dataset\ndataset_path = '../../data/train_dataset'\ncard_images, card_labels, card_class_names = load_dataset(dataset_path)\n\nif isinstance(card_images, tuple):\n    card_images, card_labels = card_images\n    card_class_names = [f\"synthetic_{i}\" for i in range(len(np.unique(card_labels)))]\n\nprint(f\"Dataset loaded: {card_images.shape[0]} images, {len(np.unique(card_labels))} classes\")\n\n# Display sample images\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfor i, ax in enumerate(axes.flatten()):\n    if i < len(card_images):\n        idx = np.random.randint(0, len(card_images))\n        ax.imshow(card_images[idx])\n        ax.set_title(f'Class {card_labels[idx]}')\n        ax.axis('off')\n\nplt.tight_layout()\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T10:42:45.992009Z",
     "start_time": "2025-07-18T10:42:45.986439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "\n",
    "class PCAFromScratch:\n",
    "    \"\"\"\n",
    "    PCA implementation from scratch for image compression\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components, normalization='column'):\n",
    "        self.n_components = n_components\n",
    "        self.normalization = normalization\n",
    "        self.mean_ = None\n",
    "        self.components_ = None\n",
    "        self.explained_variance_ratio_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit PCA to the data\n",
    "\n",
    "        Args:\n",
    "            X: Data matrix (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        # Center the data\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean_\n",
    "\n",
    "        # Apply normalization\n",
    "        if self.normalization == 'column':\n",
    "            self.std_ = np.std(X_centered, axis=0)\n",
    "            # Avoid division by zero\n",
    "            self.std_[self.std_ == 0] = 1\n",
    "            X_normalized = X_centered / self.std_\n",
    "        elif self.normalization == 'full':\n",
    "            self.std_ = np.std(X_centered)\n",
    "            X_normalized = X_centered / self.std_\n",
    "        else:\n",
    "            X_normalized = X_centered\n",
    "            self.std_ = 1\n",
    "\n",
    "        # Compute covariance matrix\n",
    "        cov_matrix = np.cov(X_normalized.T)\n",
    "\n",
    "        # Compute eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "        # Sort eigenvalues and eigenvectors in descending order\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "        # Select top n_components\n",
    "        self.components_ = eigenvectors[:, :self.n_components].T\n",
    "        self.eigenvalues_ = eigenvalues[:self.n_components]\n",
    "\n",
    "        # Calculate explained variance ratio\n",
    "        total_variance = np.sum(eigenvalues)\n",
    "        self.explained_variance_ratio_ = self.eigenvalues_ / total_variance\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data to PCA space\n",
    "\n",
    "        Args:\n",
    "            X: Data matrix (n_samples, n_features)\n",
    "\n",
    "        Returns:\n",
    "            X_transformed: Transformed data (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        # Center the data\n",
    "        X_centered = X - self.mean_\n",
    "\n",
    "        # Apply normalization\n",
    "        if self.normalization == 'column':\n",
    "            X_normalized = X_centered / self.std_\n",
    "        elif self.normalization == 'full':\n",
    "            X_normalized = X_centered / self.std_\n",
    "        else:\n",
    "            X_normalized = X_centered\n",
    "\n",
    "        # Project onto principal components\n",
    "        return np.dot(X_normalized, self.components_.T)\n",
    "\n",
    "    def inverse_transform(self, X_transformed):\n",
    "        \"\"\"\n",
    "        Transform data back to original space\n",
    "\n",
    "        Args:\n",
    "            X_transformed: Transformed data (n_samples, n_components)\n",
    "\n",
    "        Returns:\n",
    "            X_reconstructed: Reconstructed data (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        # Project back to original space\n",
    "        X_reconstructed = np.dot(X_transformed, self.components_)\n",
    "\n",
    "        # Reverse normalization\n",
    "        if self.normalization == 'column':\n",
    "            X_reconstructed = X_reconstructed * self.std_\n",
    "        elif self.normalization == 'full':\n",
    "            X_reconstructed = X_reconstructed * self.std_\n",
    "\n",
    "        # Add back the mean\n",
    "        return X_reconstructed + self.mean_\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit PCA and transform data\n",
    "        \"\"\"\n",
    "        return self.fit(X).transform(X)"
   ],
   "id": "58810434d857d373",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:42:15.076883Z",
     "start_time": "2025-07-18T09:42:14.230318Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef pca_compress_images(images, n_components, normalization='column'):\n    \"\"\"Compress images using PCA\"\"\"\n    if isinstance(images, list):\n        images = np.array(images)\n    \n    # Flatten images\n    original_shape = images.shape\n    flattened = images.reshape(images.shape[0], -1)\n    \n    # Center data\n    mean = np.mean(flattened, axis=0)\n    centered = flattened - mean\n    \n    # Normalize\n    if normalization == 'column':\n        std = np.std(centered, axis=0)\n        std[std == 0] = 1\n        normalized = centered / std\n    else:\n        std = np.std(centered)\n        normalized = centered / std\n    \n    # PCA\n    cov_matrix = np.cov(normalized.T)\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    \n    # Sort descending\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    \n    # Select components\n    components = eigenvectors[:, :n_components].T\n    \n    # Transform and reconstruct\n    transformed = np.dot(normalized, components.T)\n    reconstructed = np.dot(transformed, components)\n    \n    # Reverse normalization\n    if normalization == 'column':\n        reconstructed = reconstructed * std + mean\n    else:\n        reconstructed = reconstructed * std + mean\n    \n    # Reshape back\n    reconstructed = reconstructed.reshape(original_shape)\n    reconstructed = np.clip(reconstructed, 0, 255).astype(np.uint8)\n    \n    compression_ratio = flattened.shape[1] / n_components\n    variance_ratio = np.sum(eigenvalues[:n_components]) / np.sum(eigenvalues)\n    \n    return reconstructed, compression_ratio, variance_ratio\n\ndef show_compression_comparison(original_images, component_counts, num_examples=5):\n    \"\"\"Show compression comparison\"\"\"\n    num_examples = min(num_examples, len(original_images))\n    \n    fig, axes = plt.subplots(len(component_counts) + 1, num_examples, \n                           figsize=(4 * num_examples, 4 * (len(component_counts) + 1)))\n    \n    if len(component_counts) == 0:\n        axes = axes.reshape(1, -1)\n    elif num_examples == 1:\n        axes = axes.reshape(-1, 1)\n    \n    # Show originals\n    for i in range(num_examples):\n        axes[0, i].imshow(original_images[i])\n        axes[0, i].set_title(f'Original\\n{64*64*3:,} dims')\n        axes[0, i].axis('off')\n    \n    # Show compressed versions\n    for row, n_comp in enumerate(component_counts, 1):\n        compressed_imgs, ratio, variance = pca_compress_images(\n            original_images[:num_examples], n_comp)\n        \n        for i in range(num_examples):\n            axes[row, i].imshow(compressed_imgs[i])\n            axes[row, i].set_title(f'{n_comp} comp.\\n{ratio:.1f}x compression\\n{variance:.1%} variance')\n            axes[row, i].axis('off')\n    \n    plt.suptitle('PCA Image Compression Comparison', fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\ndef compress_and_decompress_images(test_images, training_images, n_components):\n    \"\"\"Compress and decompress using separate training set\"\"\"\n    # Flatten\n    X_train = np.array([img.flatten() for img in training_images])\n    X_test = np.array([img.flatten() for img in test_images])\n    \n    # Fit PCA on training data\n    mean = np.mean(X_train, axis=0)\n    centered = X_train - mean\n    std = np.std(centered)\n    normalized = centered / std\n    \n    cov_matrix = np.cov(normalized.T)\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    \n    idx = np.argsort(eigenvalues)[::-1]\n    components = eigenvectors[:, idx][:, :n_components].T\n    \n    # Transform test data\n    test_centered = X_test - mean\n    test_normalized = test_centered / std\n    compressed = np.dot(test_normalized, components.T)\n    reconstructed = np.dot(compressed, components)\n    reconstructed = (reconstructed * std + mean).reshape(-1, 64, 64, 3)\n    reconstructed = np.clip(reconstructed, 0, 255).astype(np.uint8)\n    \n    return compressed, reconstructed\n\ndef show_compression_pipeline(original_images, training_images, n_components):\n    \"\"\"Show compression pipeline\"\"\"\n    compressed_data, reconstructed_images = compress_and_decompress_images(\n        original_images, training_images, n_components)\n    \n    num_examples = len(original_images)\n    fig, axes = plt.subplots(2, num_examples, figsize=(4 * num_examples, 8))\n    \n    if num_examples == 1:\n        axes = axes.reshape(-1, 1)\n    \n    for i in range(num_examples):\n        axes[0, i].imshow(original_images[i])\n        axes[0, i].set_title(\"Original\")\n        axes[0, i].axis('off')\n        \n        axes[1, i].imshow(reconstructed_images[i])\n        axes[1, i].set_title(\"Reconstructed\")\n        axes[1, i].axis('off')\n    \n    compression_ratio = (64 * 64 * 3) / n_components\n    plt.suptitle(f'Compression Pipeline - {n_components} components\\n{compression_ratio:.1f}x compression')\n    plt.tight_layout()\n    plt.show()\n    \n    return compressed_data, reconstructed_images",
   "id": "32e975a68ccc3ffb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:56:23.318987Z",
     "start_time": "2025-07-18T09:45:38.809454Z"
    }
   },
   "cell_type": "code",
   "source": "# Test compression with different component counts\ncomponent_counts = [10, 25, 50, 100, 200]\nshow_compression_comparison(card_images[:5], component_counts, num_examples=5)",
   "id": "994a928f18d0f0c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-18T10:13:49.723588Z"
    }
   },
   "cell_type": "code",
   "source": "# Test compression pipeline\ntest_samples = card_images[:5]\ntraining_samples = card_images[100:1000]\n\ncompressed_data, reconstructed_images = show_compression_pipeline(\n    test_samples, training_samples, n_components=50)\n\n# Calculate metrics\noriginal_array = np.array(test_samples)\nreconstructed_array = np.array(reconstructed_images)\nmse = np.mean((original_array.astype(float) - reconstructed_array.astype(float)) ** 2)\npsnr = 20 * np.log10(255.0 / np.sqrt(mse)) if mse > 0 else float('inf')\n\nprint(f\"Compression metrics:\")\nprint(f\"  - MSE: {mse:.2f}\")\nprint(f\"  - PSNR: {psnr:.2f} dB\")\nprint(f\"  - Compression ratio: {(64*64*3)/50:.1f}x\")",
   "id": "4294916efd52285a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T10:47:21.771305Z",
     "start_time": "2025-07-18T10:43:01.530656Z"
    }
   },
   "cell_type": "code",
   "source": "# PCA Visualization\ndef create_pca_scatter(images, labels, n_components=2, sample_size=1000):\n    \"\"\"Create PCA scatter plot\"\"\"\n    actual_size = min(sample_size, len(images))\n    X_sample = np.array([img.flatten() for img in images[:actual_size]])\n    y_sample = labels[:actual_size]\n    \n    # Apply PCA\n    mean = np.mean(X_sample, axis=0)\n    centered = X_sample - mean\n    std = np.std(centered)\n    normalized = centered / std\n    \n    cov_matrix = np.cov(normalized.T)\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    \n    idx = np.argsort(eigenvalues)[::-1]\n    components = eigenvectors[:, idx][:, :n_components].T\n    X_reduced = np.dot(normalized, components.T)\n    \n    # Plot\n    plt.figure(figsize=(10, 8))\n    unique_classes = np.unique(y_sample)\n    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_classes)))\n    \n    for i, class_label in enumerate(unique_classes):\n        mask = y_sample == class_label\n        plt.scatter(X_reduced[mask, 0], X_reduced[mask, 1], \n                   c=[colors[i]], label=f'Class {class_label}', alpha=0.7)\n    \n    variance_ratio = np.sum(eigenvalues[:n_components]) / np.sum(eigenvalues)\n    plt.xlabel('First Principal Component')\n    plt.ylabel('Second Principal Component')\n    plt.title(f'PCA Visualization - {variance_ratio:.1%} variance explained')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    return X_reduced\n\n# Create PCA visualization\nX_2d = create_pca_scatter(card_images, card_labels, sample_size=2000)",
   "id": "626fbd08fb98225e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T10:49:46.567619Z",
     "start_time": "2025-07-18T10:47:36.236710Z"
    }
   },
   "cell_type": "code",
   "source": "# Additional PCA analysis with specific classes\ndef analyze_specific_classes(images, labels, selected_classes=None):\n    \"\"\"Analyze specific classes with PCA\"\"\"\n    if selected_classes is None:\n        selected_classes = np.unique(labels)[:10]  # First 10 classes\n    \n    mask = np.isin(labels, selected_classes)\n    filtered_images = images[mask]\n    filtered_labels = labels[mask]\n    \n    # Sample up to 50 per class\n    sample_indices = []\n    for class_label in selected_classes:\n        class_indices = np.where(filtered_labels == class_label)[0]\n        if len(class_indices) > 0:\n            selected = np.random.choice(class_indices, min(50, len(class_indices)), replace=False)\n            sample_indices.extend(selected)\n    \n    X_sample = np.array([filtered_images[i].flatten() for i in sample_indices])\n    y_sample = filtered_labels[sample_indices]\n    \n    # Apply PCA\n    mean = np.mean(X_sample, axis=0)\n    centered = X_sample - mean\n    std = np.std(centered)\n    normalized = centered / std\n    \n    cov_matrix = np.cov(normalized.T)\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    \n    idx = np.argsort(eigenvalues)[::-1]\n    components = eigenvectors[:, idx][:, :2].T\n    X_2d = np.dot(normalized, components.T)\n    \n    # Plot\n    plt.figure(figsize=(12, 8))\n    colors = plt.cm.tab10(np.linspace(0, 1, len(selected_classes)))\n    \n    for i, class_label in enumerate(selected_classes):\n        mask = y_sample == class_label\n        if np.any(mask):\n            plt.scatter(X_2d[mask, 0], X_2d[mask, 1], \n                       c=[colors[i]], label=f'Class {class_label}', alpha=0.7)\n    \n    variance_ratio = np.sum(eigenvalues[:2]) / np.sum(eigenvalues)\n    plt.xlabel('First Principal Component')\n    plt.ylabel('Second Principal Component')\n    plt.title(f'Class-Specific PCA Analysis - {variance_ratio:.1%} variance explained')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    return X_2d\n\nX_2d_classes = analyze_specific_classes(card_images, card_labels)",
   "id": "1e34d5d15f1f66f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# End of notebook",
   "id": "271d11e79045d5f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}