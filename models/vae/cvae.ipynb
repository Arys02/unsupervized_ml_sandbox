{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T06:37:57.997829Z",
     "start_time": "2025-07-18T06:37:53.547114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from config import RAW_DATA_DIR\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from IPython.display import HTML"
   ],
   "id": "4ad253022871afbf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-18 08:37:54.618\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mconfig\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m9\u001B[0m - \u001B[1mPROJ_ROOT path is: /home/arys/projects/unsupervized_ml_sandbox\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T06:37:58.550720Z",
     "start_time": "2025-07-18T06:37:58.321473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "### Prep Dataset ###\n",
    "tensor_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((28, 28)),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root=RAW_DATA_DIR / 'mnist', train=True, transform=tensor_transforms,\n",
    "                                           download=True)\n",
    "val_dataset = torchvision.datasets.MNIST(root=RAW_DATA_DIR / 'mnist', train=False, transform=tensor_transforms,\n",
    "                                         download=True)\n",
    "batch_size = 1024\n",
    "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=8)\n",
    "val_dl = torch.utils.data.DataLoader(val_dataset, batch_size=1024, shuffle=True, num_workers=8)\n",
    "\n",
    "### Set Device ###\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "id": "4d6e777cc15147cc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T06:37:58.567510Z",
     "start_time": "2025-07-18T06:37:58.562255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=2, input_size=28 * 28):\n",
    "        super().__init__()\n",
    "        self.input_size: int = input_size\n",
    "        self.latent_dim: int = latent_dim\n",
    "\n",
    "\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "\n",
    "            ### Convolutional Encoding ###\n",
    "            nn.Conv2d(in_channels=input_size, out_channels=8, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=self.latent_dim, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.latent_dim),\n",
    "            nn.ReLU(),\n",
    "\n",
    "\n",
    "        )\n",
    "\n",
    "        #########################################################\n",
    "        ### The New Layers Added in from Original AutoEncoder ###\n",
    "        self.conv_mu =  nn.Conv2d(in_channels=self.latent_dim, out_channels=self.latent_dim, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.conv_logvar = nn.Conv2d(in_channels=self.latent_dim, out_channels=self.latent_dim, kernel_size=3, stride=1, padding=\"same\")\n",
    "        #########################################################\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=self.latent_dim, out_channels=16, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=input_size, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward_dec(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward_enc(self, x):\n",
    "        conv_enc = self.encoder_conv(x)\n",
    "\n",
    "        #############################################\n",
    "        ### Compute Mu and Sigma ###\n",
    "        mu = self.conv_mu(conv_enc)\n",
    "        logvar = self.conv_logvar(conv_enc)\n",
    "\n",
    "        ### Sample with Reparamaterization Trick ###\n",
    "        sigma = torch.exp(0.5*logvar)\n",
    "        noise = torch.randn_like(sigma, device=sigma.device)\n",
    "        z = mu + sigma*noise\n",
    "        ############################################\n",
    "\n",
    "        return z, mu, logvar\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.forward_enc(x)\n",
    "\n",
    "        return z, self.decoder(z), mu, logvar\n",
    "\n"
   ],
   "id": "3d5755720d3748eb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T06:37:58.677933Z",
     "start_time": "2025-07-18T06:37:58.615384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def VAELoss(x, x_hat, mean, log_var, kl_weight=1, reconstruction_weight=1):\n",
    "    pixel_mse = ((x - x_hat) ** 2)\n",
    "\n",
    "    reconstruction_loss = pixel_mse.sum(axis=-1).mean()\n",
    "    # reconstruction_loss = pixel_mse.mean()\n",
    "\n",
    "    kl = (1 + log_var - mean ** 2 - torch.exp(log_var))\n",
    "\n",
    "    kl_per_image = -0.5 * torch.sum(kl, dim=-1)\n",
    "\n",
    "    kl_loss = torch.mean(kl_per_image)\n",
    "    #print(reconstruction_loss, kl_loss)\n",
    "\n",
    "    return reconstruction_loss * reconstruction_weight + kl_weight * kl_loss\n",
    "\n",
    "\n",
    "x = torch.randn(4, 128)\n",
    "x_hat = torch.randn(4, 128)\n",
    "\n",
    "mean = torch.randn(4, 2)\n",
    "log_var = torch.randn(4, 2)\n",
    "\n",
    "VAELoss(x, x_hat, mean, log_var)\n"
   ],
   "id": "3f05ed16896e6932",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(289.8410)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Trainning",
   "id": "9160e07a08eed78f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T06:37:58.707194Z",
     "start_time": "2025-07-18T06:37:58.702617Z"
    }
   },
   "cell_type": "code",
   "source": "val_dataset.data.shape[1]",
   "id": "c9272fe57a3c990a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T06:37:58.775738Z",
     "start_time": "2025-07-18T06:37:58.765321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from loguru import logger\n",
    "\n",
    "mlflow.set_experiment(\"unsupervized_ml_sandbox\")\n",
    "\n",
    "logger.info(f\"model_name : VAE\")\n",
    "\n"
   ],
   "id": "5cf59f703e8cddaf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-18 08:37:58.773\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m5\u001B[0m - \u001B[1mmodel_name : VAE\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T06:37:58.828171Z",
     "start_time": "2025-07-18T06:37:58.825603Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "75cdee3735a01cb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-18T06:37:58.883002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with mlflow.start_run(run_name=\"vae\"):\n",
    "    kl_weight = 1\n",
    "    input_size = val_dataset.data.shape[1] ** 2\n",
    "    epochs = 1000\n",
    "    model = VAE(latent_dim=2, input_size=input_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    val_step = 10\n",
    "\n",
    "    mlflow.log_param(\"kl_weight\", kl_weight)\n",
    "    mlflow.log_param(\"input_size\", input_size)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_param(\"model\", model)\n",
    "    mlflow.log_param(\"optimizer\", optimizer)\n",
    "    mlflow.log_param(\"val_step\", val_step)\n",
    "\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    train_loss = []\n",
    "\n",
    "    encoded_data_per_eval = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train = True\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss_epoch = []\n",
    "\n",
    "        for imgs, _ in train_dl:\n",
    "            imgs = imgs.to(device)\n",
    "            imgs = imgs.flatten(1)\n",
    "\n",
    "            encoded, decoded, mu, logvar = model(imgs)\n",
    "\n",
    "            loss = VAELoss(imgs, decoded, mu, logvar, kl_weight=kl_weight)\n",
    "            train_loss_epoch.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "        avg_train_loss = np.mean(train_loss_epoch)\n",
    "\n",
    "        if epoch % val_step == 0:\n",
    "            model.eval()\n",
    "            val_loss_epoch = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for img, _ in val_dl:\n",
    "                    img = img.to(device)\n",
    "                    img = img.flatten(1)\n",
    "                    encoded, decoded, mu, logvar = model(img)\n",
    "                    loss = VAELoss(img, decoded, mu, logvar, kl_weight=kl_weight)\n",
    "                    val_loss_epoch.append(loss.item())\n",
    "\n",
    "            avg_val_loss = np.mean(val_loss_epoch)\n",
    "\n",
    "            print(f\"Epoch {epoch} — train_loss: {avg_train_loss:.6f} — val_loss: {avg_val_loss:.6f}\")\n",
    "            mlflow.log_metric(\"val_loss\", avg_train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"train_loss\", avg_val_loss, step=epoch)\n",
    "            # on sauvegarde les moyennes\n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "8eafe40c4d9258a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83d8bfd2819549bb9d8555bae24ed499"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 — train_loss: 181.805996 — val_loss: 181.868721\n",
      "Epoch 10 — train_loss: 181.805536 — val_loss: 181.866811\n",
      "Epoch 20 — train_loss: 181.804031 — val_loss: 181.871133\n",
      "Epoch 30 — train_loss: 181.806849 — val_loss: 181.870490\n",
      "Epoch 40 — train_loss: 181.805868 — val_loss: 181.872772\n",
      "Epoch 50 — train_loss: 181.805077 — val_loss: 181.870129\n",
      "Epoch 60 — train_loss: 181.804157 — val_loss: 181.868692\n",
      "Epoch 70 — train_loss: 181.806827 — val_loss: 181.867186\n",
      "Epoch 80 — train_loss: 181.803819 — val_loss: 181.867885\n",
      "Epoch 90 — train_loss: 181.804091 — val_loss: 181.874025\n",
      "Epoch 100 — train_loss: 181.805730 — val_loss: 181.867329\n",
      "Epoch 110 — train_loss: 181.806418 — val_loss: 181.872432\n",
      "Epoch 120 — train_loss: 181.806146 — val_loss: 181.867928\n",
      "Epoch 130 — train_loss: 181.805069 — val_loss: 181.868866\n",
      "Epoch 140 — train_loss: 181.805601 — val_loss: 181.861818\n",
      "Epoch 150 — train_loss: 181.804605 — val_loss: 181.873141\n",
      "Epoch 160 — train_loss: 181.804865 — val_loss: 181.867365\n",
      "Epoch 170 — train_loss: 181.803071 — val_loss: 181.871364\n",
      "Epoch 180 — train_loss: 181.804670 — val_loss: 181.870778\n",
      "Epoch 190 — train_loss: 181.805644 — val_loss: 181.866690\n",
      "Epoch 200 — train_loss: 181.804143 — val_loss: 181.870714\n",
      "Epoch 210 — train_loss: 181.807552 — val_loss: 181.880748\n",
      "Epoch 220 — train_loss: 181.805709 — val_loss: 181.864540\n",
      "Epoch 230 — train_loss: 181.805479 — val_loss: 181.868404\n",
      "Epoch 240 — train_loss: 181.807670 — val_loss: 181.868935\n",
      "Epoch 250 — train_loss: 181.806210 — val_loss: 181.868236\n",
      "Epoch 260 — train_loss: 181.804393 — val_loss: 181.866519\n",
      "Epoch 270 — train_loss: 181.804423 — val_loss: 181.869655\n",
      "Epoch 280 — train_loss: 181.803460 — val_loss: 181.871089\n",
      "Epoch 290 — train_loss: 181.805965 — val_loss: 181.865929\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T06:13:50.842587Z",
     "start_time": "2025-07-18T06:13:50.837223Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "6379fc2944263ae3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "  )\n",
       "  (fn_mu): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (fn_logvar): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=128, out_features=784, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1438fa3d8a930ce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
